{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taotl\\AppData\\Roaming\\Python\\Python37\\site-packages\\IPython\\html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n",
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scio\n",
    "import dgl\n",
    "import torch\n",
    "from dgl.data import DGLDataset\n",
    "from dgl import save_graphs, load_graphs\n",
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "import sklearn\n",
    "%matplotlib inline\n",
    "    \n",
    "\n",
    "class MyDataset(DGLDataset):\n",
    "    def __init__(self,\n",
    "                 url=None,\n",
    "                 raw_dir=None,\n",
    "                 save_dir=None,\n",
    "                 force_reload=False,\n",
    "                 verbose=False):\n",
    "        super(MyDataset, self).__init__(name='dataset_name',\n",
    "                                        url=url,\n",
    "                                        raw_dir=raw_dir,\n",
    "                                        save_dir=save_dir,\n",
    "                                        force_reload=force_reload,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        self.graphs = []\n",
    "        self.labels = []\n",
    "        for file_idx in range(len(data)):\n",
    "            file_name = data[file_idx].replace('\\n', '')\n",
    "            if os.path.exists(file_name):\n",
    "                mat_data = scio.loadmat(file_name)\n",
    "                roi_signal = mat_data['SignalMatrix2d']\n",
    "                roi_signal[abs(roi_signal)<threshold]=0\n",
    "                \n",
    "                ndata = mat_data['SignalMatrix3d']\n",
    "                # print(ndata[0:22,:].shape)\n",
    "                r, w = roi_signal.shape\n",
    "                \n",
    "                src = []\n",
    "                dst = []\n",
    "                edata = []\n",
    "                # print(file_name)\n",
    "                for i in range(r):\n",
    "                    for j in range(w):\n",
    "                        if roi_signal[i, j]>0:\n",
    "                            src.append(i)\n",
    "                            dst.append(j)\n",
    "                            edata.append([roi_signal[i, j]])\n",
    "                # print(file_idx)\n",
    "                graph = dgl.graph((src, dst))\n",
    "                # print(len(src),len(dst))\n",
    "                # print(graph.num_nodes())\n",
    "                \n",
    "                graph.edata['w']=torch.tensor(edata).float()\n",
    "                # print(graph.edata['w'].size())\n",
    "                graph.ndata['w']=torch.tensor(ndata[0:graph.num_nodes(),:]).float()\n",
    "                # print(len(ndata))\n",
    "                # print(graph.ndata['w'].size())\n",
    "                \n",
    "                self.graphs.append(graph)\n",
    "                self.labels.append(torch.tensor(int(mat_data['Signal_label'])))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.graphs[idx], self.labels[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "874\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.8\n",
    "os.chdir(\"D:\\CHB seizure\\GIH data\") \n",
    "with open(\"D:\\CHB seizure\\GIH data\\path.txt\", \"r\") as f:\n",
    "    data = f.readlines()    \n",
    "dataset = MyDataset()\n",
    "\n",
    "\n",
    "# os.chdir(\"D:\\CHB seizure\\GIN_rebuttal_data\") \n",
    "# with open(\"D:\\CHB seizure\\GIN_rebuttal_data\\path.txt\", \"r\") as f:\n",
    "#     data = f.readlines()    \n",
    "# rebuttal_dataset = MyDataset()\n",
    "# print(len(rebuttal_dataset))\n",
    "\n",
    "print(len(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GINConv\n",
    "from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n",
    "\n",
    "\n",
    "class ApplyNodeFunc(nn.Module):\n",
    "    \"\"\"Update the node feature hv with MLP, BN and ReLU.\"\"\"\n",
    "    def __init__(self, mlp):\n",
    "        super(ApplyNodeFunc, self).__init__()\n",
    "        self.mlp = mlp\n",
    "        self.bn = nn.BatchNorm1d(self.mlp.output_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.mlp(h)\n",
    "        h = self.bn(h)\n",
    "        h = F.relu(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP with linear output\"\"\"\n",
    "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_or_not = True  # default is linear model\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if num_layers < 1:\n",
    "            raise ValueError(\"number of layers should be positive!\")\n",
    "        elif num_layers == 1:\n",
    "            # Linear model\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "        else:\n",
    "            # Multi-layer model\n",
    "            self.linear_or_not = False\n",
    "            self.linears = torch.nn.ModuleList()\n",
    "            self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
    "            for layer in range(num_layers - 2):\n",
    "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "            for layer in range(num_layers - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.linear_or_not:\n",
    "            # If linear model\n",
    "            return self.linear(x)\n",
    "        else:\n",
    "            # If MLP\n",
    "            h = x\n",
    "            for i in range(self.num_layers - 1):\n",
    "                h = F.relu(self.batch_norms[i](self.linears[i](h)))\n",
    "            return self.linears[-1](h)\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    \"\"\"GIN model\"\"\"\n",
    "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim,\n",
    "                 output_dim, final_dropout, learn_eps, graph_pooling_type,\n",
    "                 neighbor_pooling_type):\n",
    "\n",
    "        super(GIN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.learn_eps = learn_eps\n",
    "\n",
    "        # List of MLPs\n",
    "        self.ginlayers = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            if layer == 0:\n",
    "                mlp = MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim)\n",
    "            else:\n",
    "                mlp = MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n",
    "\n",
    "            self.ginlayers.append(\n",
    "                GINConv(ApplyNodeFunc(mlp), neighbor_pooling_type, 0, self.learn_eps))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Linear function for graph poolings of output of each layer\n",
    "        # which maps the output of different layers into a prediction score\n",
    "        self.linears_prediction = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                self.linears_prediction.append(\n",
    "                    nn.Linear(input_dim, output_dim))\n",
    "            else:\n",
    "                self.linears_prediction.append(\n",
    "                    nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.drop = nn.Dropout(final_dropout)\n",
    "\n",
    "        if graph_pooling_type == 'sum':\n",
    "            self.pool = SumPooling()\n",
    "        elif graph_pooling_type == 'mean':\n",
    "            self.pool = AvgPooling()\n",
    "        elif graph_pooling_type == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # list of hidden representation at each layer (including input)\n",
    "        hidden_rep = [h]\n",
    "\n",
    "        for i in range(self.num_layers - 1):\n",
    "            h = self.ginlayers[i](g, h)\n",
    "            h = self.batch_norms[i](h)\n",
    "            h = F.relu(h)\n",
    "            hidden_rep.append(h)\n",
    "\n",
    "        score_over_layer = 0\n",
    "\n",
    "        # perform pooling over all nodes in each graph in every layer\n",
    "        for i, h in enumerate(hidden_rep):\n",
    "            pooled_h = self.pool(g, h)\n",
    "            score_over_layer += self.drop(self.linears_prediction[i](pooled_h))\n",
    "\n",
    "        return score_over_layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import dgl\n",
    "from dgl.dataloading import GraphDataLoader\n",
    "\n",
    "\n",
    "class GINDataLoader():\n",
    "    def __init__(self,\n",
    "                 dataset,\n",
    "                 batch_size,\n",
    "                 device,\n",
    "                 collate_fn=None,\n",
    "                 seed=0,\n",
    "                 shuffle=True,\n",
    "                 split_name='rand',\n",
    "                 fold_idx=0,\n",
    "                 split_ratio=0.7):\n",
    "\n",
    "        self.shuffle = shuffle\n",
    "        self.seed = seed\n",
    "        self.kwargs = {'pin_memory': True} if 'cuda' in device.type else {}\n",
    "\n",
    "        labels = [l for _, l in dataset]\n",
    "\n",
    "        if split_name == 'fold10':\n",
    "            train_idx, valid_idx = self._split_fold10(\n",
    "                labels, fold_idx, seed, shuffle)\n",
    "        elif split_name == 'rand':\n",
    "            train_idx, valid_idx,test_idx = self._split_rand(\n",
    "                labels, split_ratio, seed, shuffle)\n",
    "            test_sampler = SubsetRandomSampler(test_idx)\n",
    "            self.test_loader = GraphDataLoader(\n",
    "                dataset, sampler=test_sampler,\n",
    "                batch_size=batch_size, collate_fn=collate_fn, **self.kwargs)\n",
    "        else:\n",
    "            raise NotImplementedError()\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "        \n",
    "\n",
    "        self.train_loader = GraphDataLoader(\n",
    "            dataset, sampler=train_sampler,\n",
    "            batch_size=batch_size, collate_fn=collate_fn, **self.kwargs)\n",
    "        self.valid_loader = GraphDataLoader(\n",
    "            dataset, sampler=valid_sampler,\n",
    "            batch_size=batch_size, collate_fn=collate_fn, **self.kwargs)\n",
    "\n",
    "\n",
    "    def train_valid_loader(self):\n",
    "        return self.train_loader, self.valid_loader, self.test_loader\n",
    "#         return self.train_loader, self.valid_loader\n",
    "\n",
    "    def _split_fold10(self, labels, fold_idx=0, seed=0, shuffle=True):\n",
    "        ''' 10 flod '''\n",
    "        assert 0 <= fold_idx and fold_idx < 10, print(\n",
    "            \"fold_idx must be from 0 to 9.\")\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=10, shuffle=shuffle, random_state=seed)\n",
    "        idx_list = []\n",
    "        for idx in skf.split(np.zeros(len(labels)), labels):    # split(x, y)\n",
    "            idx_list.append(idx)\n",
    "        train_idx, valid_idx = idx_list[fold_idx]\n",
    "\n",
    "        print(\n",
    "            \"train_set : test_set = %d : %d\",\n",
    "            len(train_idx), len(valid_idx))\n",
    "\n",
    "        return train_idx, valid_idx\n",
    "\n",
    "    def _split_rand(self, labels, split_ratio=0.7, seed=0, shuffle=True):\n",
    "        num_entries = len(labels)\n",
    "        indices = list(range(num_entries))\n",
    "        np.random.seed(seed)\n",
    "        np.random.shuffle(indices)\n",
    "        split = int(math.floor(split_ratio * num_entries))\n",
    "        test_split = int(math.floor(0.9 * num_entries))\n",
    "#         添加test_idx\n",
    "        train_idx, valid_idx, test_idx = indices[:split], indices[split:test_split], indices[test_split:]\n",
    "\n",
    "        print(\n",
    "            \"train_set : test_set = %d : %d\",\n",
    "            len(train_idx), len(valid_idx))\n",
    "        \n",
    "        return train_idx, valid_idx,test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is No. 1  turn\n",
      "train_set : test_set = %d : %d 611 175\n",
      "epoch: 1, train loss: 22.5798, accuracy: 50.90%\t\tvalid loss: 17.3975, accuracy: 46.29%\n",
      "valid sensitivity 87.82,specificity 12.37, precision 51.12\n",
      "test loss: 20.5435, accuracy: 48.86%\n",
      "epoch: 2, train loss: 17.2828, accuracy: 52.70%\t\tvalid loss: 13.5363, accuracy: 48.57%\n",
      "valid sensitivity 87.82,specificity 16.05, precision 52.19\n",
      "test loss: 15.7984, accuracy: 52.27%\n",
      "epoch: 3, train loss: 9.9208, accuracy: 53.68%\t\tvalid loss: 7.6046, accuracy: 56.00%\n",
      "valid sensitivity 77.88,specificity 28.43, precision 53.17\n",
      "test loss: 7.5802, accuracy: 55.68%\n",
      "epoch: 4, train loss: 5.7304, accuracy: 64.32%\t\tvalid loss: 5.0618, accuracy: 64.57%\n",
      "valid sensitivity 88.78,specificity 38.80, precision 60.22\n",
      "test loss: 4.1492, accuracy: 64.77%\n",
      "epoch: 5, train loss: 4.0410, accuracy: 73.16%\t\tvalid loss: 3.7626, accuracy: 74.29%\n",
      "valid sensitivity 92.63,specificity 52.84, precision 67.21\n",
      "test loss: 2.8468, accuracy: 69.32%\n",
      "epoch: 6, train loss: 2.6377, accuracy: 80.36%\t\tvalid loss: 2.1587, accuracy: 80.57%\n",
      "valid sensitivity 95.83,specificity 64.21, precision 73.65\n",
      "test loss: 1.4664, accuracy: 80.68%\n",
      "epoch: 7, train loss: 3.0448, accuracy: 80.52%\t\tvalid loss: 2.3490, accuracy: 81.14%\n",
      "valid sensitivity 99.68,specificity 60.54, precision 72.49\n",
      "test loss: 1.7767, accuracy: 77.27%\n",
      "epoch: 8, train loss: 2.1124, accuracy: 83.80%\t\tvalid loss: 1.9298, accuracy: 82.29%\n",
      "valid sensitivity 93.27,specificity 73.91, precision 78.86\n",
      "test loss: 1.3494, accuracy: 84.09%\n",
      "epoch: 9, train loss: 3.1903, accuracy: 81.67%\t\tvalid loss: 2.5783, accuracy: 81.71%\n",
      "valid sensitivity 98.08,specificity 64.55, precision 74.27\n",
      "test loss: 2.1318, accuracy: 80.68%\n",
      "epoch: 10, train loss: 1.8288, accuracy: 85.27%\t\tvalid loss: 1.3018, accuracy: 84.57%\n",
      "valid sensitivity 87.18,specificity 83.28, precision 84.47\n",
      "test loss: 1.0739, accuracy: 84.09%\n",
      "epoch: 11, train loss: 1.3885, accuracy: 86.58%\t\tvalid loss: 1.1228, accuracy: 87.43%\n",
      "valid sensitivity 94.23,specificity 78.60, precision 82.12\n",
      "test loss: 0.7253, accuracy: 84.09%\n",
      "epoch: 12, train loss: 1.1126, accuracy: 88.71%\t\tvalid loss: 0.8531, accuracy: 90.29%\n",
      "valid sensitivity 90.38,specificity 86.96, precision 87.85\n",
      "test loss: 0.5317, accuracy: 87.50%\n",
      "epoch: 13, train loss: 1.2172, accuracy: 88.22%\t\tvalid loss: 0.7112, accuracy: 92.57%\n",
      "valid sensitivity 97.76,specificity 78.26, precision 82.43\n",
      "test loss: 0.5369, accuracy: 88.64%\n",
      "epoch: 14, train loss: 1.1815, accuracy: 87.07%\t\tvalid loss: 0.3621, accuracy: 90.86%\n",
      "valid sensitivity 83.65,specificity 90.64, precision 90.31\n",
      "test loss: 0.2757, accuracy: 89.77%\n",
      "epoch: 15, train loss: 1.8408, accuracy: 86.42%\t\tvalid loss: 0.7737, accuracy: 92.57%\n",
      "valid sensitivity 79.81,specificity 93.31, precision 92.57\n",
      "test loss: 0.9981, accuracy: 89.77%\n",
      "epoch: 16, train loss: 0.9962, accuracy: 87.73%\t\tvalid loss: 0.4261, accuracy: 93.14%\n",
      "valid sensitivity 99.68,specificity 75.25, precision 80.78\n",
      "test loss: 0.6082, accuracy: 90.91%\n",
      "epoch: 17, train loss: 0.6377, accuracy: 89.69%\t\tvalid loss: 0.3824, accuracy: 93.71%\n",
      "valid sensitivity 98.08,specificity 80.94, precision 84.30\n",
      "test loss: 0.4024, accuracy: 89.77%\n",
      "epoch: 18, train loss: 0.8174, accuracy: 91.00%\t\tvalid loss: 0.3217, accuracy: 95.43%\n",
      "valid sensitivity 98.40,specificity 83.28, precision 85.99\n",
      "test loss: 0.3525, accuracy: 92.05%\n",
      "epoch: 19, train loss: 1.1857, accuracy: 90.83%\t\tvalid loss: 0.2479, accuracy: 95.43%\n",
      "valid sensitivity 99.04,specificity 82.27, precision 85.36\n",
      "test loss: 0.3988, accuracy: 90.91%\n",
      "epoch: 20, train loss: 1.8022, accuracy: 87.07%\t\tvalid loss: 0.5869, accuracy: 92.57%\n",
      "valid sensitivity 98.72,specificity 74.92, precision 80.42\n",
      "test loss: 0.7811, accuracy: 88.64%\n",
      "epoch: 21, train loss: 1.6368, accuracy: 85.43%\t\tvalid loss: 0.9056, accuracy: 90.29%\n",
      "valid sensitivity 99.36,specificity 70.90, precision 78.09\n",
      "test loss: 1.1677, accuracy: 85.23%\n",
      "epoch: 22, train loss: 0.9310, accuracy: 88.71%\t\tvalid loss: 0.4468, accuracy: 92.57%\n",
      "valid sensitivity 98.40,specificity 78.60, precision 82.75\n",
      "test loss: 2.3245, accuracy: 88.64%\n",
      "epoch: 23, train loss: 0.7958, accuracy: 90.34%\t\tvalid loss: 0.3489, accuracy: 96.00%\n",
      "valid sensitivity 97.44,specificity 82.94, precision 85.63\n",
      "test loss: 0.4182, accuracy: 90.91%\n",
      "epoch: 24, train loss: 0.6388, accuracy: 91.16%\t\tvalid loss: 0.4542, accuracy: 93.14%\n",
      "valid sensitivity 85.58,specificity 96.99, precision 96.74\n",
      "test loss: 0.7260, accuracy: 89.77%\n",
      "epoch: 25, train loss: 0.7337, accuracy: 92.96%\t\tvalid loss: 0.3133, accuracy: 96.57%\n",
      "valid sensitivity 97.12,specificity 88.63, precision 89.91\n",
      "test loss: 0.4001, accuracy: 93.18%\n",
      "epoch: 26, train loss: 1.6672, accuracy: 85.92%\t\tvalid loss: 0.7341, accuracy: 90.86%\n",
      "valid sensitivity 99.68,specificity 71.57, precision 78.54\n",
      "test loss: 1.3123, accuracy: 86.36%\n",
      "epoch: 27, train loss: 0.8308, accuracy: 90.34%\t\tvalid loss: 0.3100, accuracy: 94.86%\n",
      "valid sensitivity 94.55,specificity 85.95, precision 87.54\n",
      "test loss: 0.3821, accuracy: 90.91%\n",
      "epoch: 28, train loss: 0.5309, accuracy: 90.18%\t\tvalid loss: 0.2386, accuracy: 92.57%\n",
      "valid sensitivity 88.14,specificity 92.31, precision 92.28\n",
      "test loss: 0.2553, accuracy: 92.05%\n",
      "epoch: 29, train loss: 0.5988, accuracy: 90.51%\t\tvalid loss: 0.4307, accuracy: 91.43%\n",
      "valid sensitivity 85.90,specificity 95.32, precision 95.04\n",
      "test loss: 0.5321, accuracy: 88.64%\n",
      "epoch: 30, train loss: 0.6521, accuracy: 90.02%\t\tvalid loss: 0.5495, accuracy: 90.86%\n",
      "valid sensitivity 83.33,specificity 96.99, precision 96.65\n",
      "test loss: 0.6007, accuracy: 89.77%\n",
      "epoch: 31, train loss: 0.3650, accuracy: 91.65%\t\tvalid loss: 0.6002, accuracy: 92.57%\n",
      "valid sensitivity 88.46,specificity 94.98, precision 94.85\n",
      "test loss: 0.1701, accuracy: 90.91%\n",
      "epoch: 32, train loss: 0.5419, accuracy: 90.67%\t\tvalid loss: 0.7212, accuracy: 92.00%\n",
      "valid sensitivity 84.62,specificity 96.99, precision 96.70\n",
      "test loss: 0.5015, accuracy: 90.91%\n",
      "epoch: 33, train loss: 0.3695, accuracy: 92.64%\t\tvalid loss: 0.3657, accuracy: 91.43%\n",
      "valid sensitivity 92.63,specificity 92.64, precision 92.93\n",
      "test loss: 0.3214, accuracy: 87.50%\n",
      "epoch: 34, train loss: 0.5188, accuracy: 91.82%\t\tvalid loss: 0.3182, accuracy: 91.43%\n",
      "valid sensitivity 88.78,specificity 94.98, precision 94.86\n",
      "test loss: 0.5143, accuracy: 89.77%\n",
      "epoch: 35, train loss: 0.7178, accuracy: 89.85%\t\tvalid loss: 0.3524, accuracy: 92.57%\n",
      "valid sensitivity 86.54,specificity 93.31, precision 93.10\n",
      "test loss: 0.4240, accuracy: 88.64%\n",
      "epoch: 36, train loss: 0.3837, accuracy: 91.00%\t\tvalid loss: 0.3662, accuracy: 92.57%\n",
      "valid sensitivity 94.23,specificity 87.63, precision 88.82\n",
      "test loss: 0.1196, accuracy: 94.32%\n",
      "epoch: 37, train loss: 0.6910, accuracy: 88.71%\t\tvalid loss: 0.7301, accuracy: 89.14%\n",
      "valid sensitivity 85.90,specificity 91.64, precision 91.47\n",
      "test loss: 0.3350, accuracy: 88.64%\n",
      "epoch: 38, train loss: 0.5583, accuracy: 90.51%\t\tvalid loss: 0.6095, accuracy: 89.71%\n",
      "valid sensitivity 87.50,specificity 93.65, precision 93.49\n",
      "test loss: 0.3495, accuracy: 89.77%\n",
      "epoch: 39, train loss: 0.3124, accuracy: 91.33%\t\tvalid loss: 0.6431, accuracy: 91.43%\n",
      "valid sensitivity 86.22,specificity 96.66, precision 96.42\n",
      "test loss: 0.2741, accuracy: 88.64%\n",
      "epoch: 40, train loss: 0.3393, accuracy: 91.49%\t\tvalid loss: 0.6150, accuracy: 89.14%\n",
      "valid sensitivity 94.23,specificity 88.63, precision 89.63\n",
      "test loss: 0.3570, accuracy: 88.64%\n",
      "epoch: 41, train loss: 0.3553, accuracy: 89.69%\t\tvalid loss: 0.5704, accuracy: 89.71%\n",
      "valid sensitivity 88.78,specificity 90.64, precision 90.82\n",
      "test loss: 0.2197, accuracy: 87.50%\n",
      "epoch: 42, train loss: 0.5373, accuracy: 90.67%\t\tvalid loss: 0.7342, accuracy: 91.43%\n",
      "valid sensitivity 83.33,specificity 98.33, precision 98.11\n",
      "test loss: 0.5985, accuracy: 89.77%\n",
      "epoch: 43, train loss: 0.2892, accuracy: 91.49%\t\tvalid loss: 0.7079, accuracy: 93.14%\n",
      "valid sensitivity 97.44,specificity 85.28, precision 87.36\n",
      "test loss: 0.5336, accuracy: 88.64%\n",
      "epoch: 44, train loss: 0.5058, accuracy: 89.69%\t\tvalid loss: 0.8724, accuracy: 89.14%\n",
      "valid sensitivity 90.38,specificity 88.96, precision 89.52\n",
      "test loss: 0.2617, accuracy: 90.91%\n",
      "epoch: 45, train loss: 0.4555, accuracy: 91.33%\t\tvalid loss: 0.8285, accuracy: 92.57%\n",
      "valid sensitivity 97.44,specificity 84.95, precision 87.11\n",
      "test loss: 0.2492, accuracy: 89.77%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 46, train loss: 0.3484, accuracy: 92.14%\t\tvalid loss: 0.7960, accuracy: 92.00%\n",
      "valid sensitivity 87.18,specificity 97.32, precision 97.14\n",
      "test loss: 0.3640, accuracy: 90.91%\n",
      "epoch: 47, train loss: 0.4013, accuracy: 91.00%\t\tvalid loss: 0.2905, accuracy: 95.43%\n",
      "valid sensitivity 99.04,specificity 82.61, precision 85.60\n",
      "test loss: 0.4082, accuracy: 90.91%\n",
      "epoch: 48, train loss: 0.2670, accuracy: 92.14%\t\tvalid loss: 0.2988, accuracy: 93.71%\n",
      "valid sensitivity 88.46,specificity 95.99, precision 95.83\n",
      "test loss: 0.2791, accuracy: 90.91%\n",
      "epoch: 49, train loss: 0.5258, accuracy: 91.16%\t\tvalid loss: 0.3341, accuracy: 90.86%\n",
      "valid sensitivity 87.18,specificity 95.32, precision 95.10\n",
      "test loss: 0.2814, accuracy: 90.91%\n",
      "epoch: 50, train loss: 0.4620, accuracy: 90.67%\t\tvalid loss: 0.2603, accuracy: 92.57%\n",
      "valid sensitivity 84.62,specificity 96.99, precision 96.70\n",
      "test loss: 0.2681, accuracy: 90.91%\n",
      "epoch: 51, train loss: 0.7212, accuracy: 90.83%\t\tvalid loss: 0.1853, accuracy: 92.57%\n",
      "valid sensitivity 88.78,specificity 92.98, precision 92.95\n",
      "test loss: 0.1598, accuracy: 92.05%\n",
      "epoch: 52, train loss: 0.7411, accuracy: 90.83%\t\tvalid loss: 0.2380, accuracy: 92.00%\n",
      "valid sensitivity 85.90,specificity 95.99, precision 95.71\n",
      "test loss: 0.2271, accuracy: 90.91%\n",
      "epoch: 53, train loss: 0.4776, accuracy: 91.16%\t\tvalid loss: 0.3351, accuracy: 91.43%\n",
      "valid sensitivity 85.58,specificity 96.99, precision 96.74\n",
      "test loss: 0.2506, accuracy: 90.91%\n",
      "epoch: 54, train loss: 0.3411, accuracy: 91.00%\t\tvalid loss: 0.2533, accuracy: 91.43%\n",
      "valid sensitivity 89.10,specificity 92.98, precision 92.98\n",
      "test loss: 0.1662, accuracy: 92.05%\n",
      "epoch: 55, train loss: 0.5255, accuracy: 91.33%\t\tvalid loss: 0.4007, accuracy: 92.00%\n",
      "valid sensitivity 84.94,specificity 97.99, precision 97.79\n",
      "test loss: 0.4374, accuracy: 89.77%\n",
      "epoch: 56, train loss: 0.3912, accuracy: 91.82%\t\tvalid loss: 0.3327, accuracy: 91.43%\n",
      "valid sensitivity 86.22,specificity 97.66, precision 97.46\n",
      "test loss: 0.2690, accuracy: 90.91%\n",
      "epoch: 57, train loss: 0.3340, accuracy: 91.82%\t\tvalid loss: 0.2728, accuracy: 92.57%\n",
      "valid sensitivity 85.90,specificity 97.99, precision 97.81\n",
      "test loss: 0.2556, accuracy: 90.91%\n",
      "epoch: 58, train loss: 0.2288, accuracy: 92.96%\t\tvalid loss: 0.2019, accuracy: 93.71%\n",
      "valid sensitivity 89.74,specificity 96.32, precision 96.22\n",
      "test loss: 0.1492, accuracy: 92.05%\n",
      "epoch: 59, train loss: 0.2502, accuracy: 91.98%\t\tvalid loss: 0.3213, accuracy: 92.00%\n",
      "valid sensitivity 86.86,specificity 97.32, precision 97.13\n",
      "test loss: 0.2108, accuracy: 90.91%\n",
      "epoch: 60, train loss: 0.1988, accuracy: 92.31%\t\tvalid loss: 0.2776, accuracy: 93.14%\n",
      "valid sensitivity 88.14,specificity 96.66, precision 96.49\n",
      "test loss: 0.1817, accuracy: 90.91%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is No. 2  turn\n",
      "train_set : test_set = %d : %d 611 175\n",
      "epoch: 1, train loss: 22.5798, accuracy: 50.90%\t\tvalid loss: 17.3975, accuracy: 46.29%\n",
      "valid sensitivity 87.82,specificity 12.37, precision 51.12\n",
      "test loss: 20.5435, accuracy: 48.86%\n",
      "epoch: 2, train loss: 17.2828, accuracy: 52.70%\t\tvalid loss: 13.5363, accuracy: 48.57%\n",
      "valid sensitivity 87.82,specificity 16.05, precision 52.19\n",
      "test loss: 15.7984, accuracy: 52.27%\n",
      "epoch: 3, train loss: 9.9208, accuracy: 53.68%\t\tvalid loss: 7.6046, accuracy: 56.00%\n",
      "valid sensitivity 77.88,specificity 28.43, precision 53.17\n",
      "test loss: 7.5802, accuracy: 55.68%\n",
      "epoch: 4, train loss: 5.7304, accuracy: 64.32%\t\tvalid loss: 5.0618, accuracy: 64.57%\n",
      "valid sensitivity 88.78,specificity 38.80, precision 60.22\n",
      "test loss: 4.1492, accuracy: 64.77%\n",
      "epoch: 5, train loss: 4.0410, accuracy: 73.16%\t\tvalid loss: 3.7626, accuracy: 74.29%\n",
      "valid sensitivity 92.63,specificity 52.84, precision 67.21\n",
      "test loss: 2.8468, accuracy: 69.32%\n",
      "epoch: 6, train loss: 2.6377, accuracy: 80.36%\t\tvalid loss: 2.1587, accuracy: 80.57%\n",
      "valid sensitivity 95.83,specificity 64.21, precision 73.65\n",
      "test loss: 1.4664, accuracy: 80.68%\n",
      "epoch: 7, train loss: 3.0448, accuracy: 80.52%\t\tvalid loss: 2.3490, accuracy: 81.14%\n",
      "valid sensitivity 99.68,specificity 60.54, precision 72.49\n",
      "test loss: 1.7767, accuracy: 77.27%\n",
      "epoch: 8, train loss: 2.1124, accuracy: 83.80%\t\tvalid loss: 1.9298, accuracy: 82.29%\n",
      "valid sensitivity 93.27,specificity 73.91, precision 78.86\n",
      "test loss: 1.3494, accuracy: 84.09%\n",
      "epoch: 9, train loss: 3.1903, accuracy: 81.67%\t\tvalid loss: 2.5783, accuracy: 81.71%\n",
      "valid sensitivity 98.08,specificity 64.55, precision 74.27\n",
      "test loss: 2.1318, accuracy: 80.68%\n",
      "epoch: 10, train loss: 1.8288, accuracy: 85.27%\t\tvalid loss: 1.3018, accuracy: 84.57%\n",
      "valid sensitivity 87.18,specificity 83.28, precision 84.47\n",
      "test loss: 1.0739, accuracy: 84.09%\n",
      "epoch: 11, train loss: 1.3885, accuracy: 86.58%\t\tvalid loss: 1.1228, accuracy: 87.43%\n",
      "valid sensitivity 94.23,specificity 78.60, precision 82.12\n",
      "test loss: 0.7253, accuracy: 84.09%\n",
      "epoch: 12, train loss: 1.1126, accuracy: 88.71%\t\tvalid loss: 0.8531, accuracy: 90.29%\n",
      "valid sensitivity 90.38,specificity 86.96, precision 87.85\n",
      "test loss: 0.5317, accuracy: 87.50%\n",
      "epoch: 13, train loss: 1.2172, accuracy: 88.22%\t\tvalid loss: 0.7112, accuracy: 92.57%\n",
      "valid sensitivity 97.76,specificity 78.26, precision 82.43\n",
      "test loss: 0.5369, accuracy: 88.64%\n",
      "epoch: 14, train loss: 1.1815, accuracy: 87.07%\t\tvalid loss: 0.3621, accuracy: 90.86%\n",
      "valid sensitivity 83.65,specificity 90.64, precision 90.31\n",
      "test loss: 0.2757, accuracy: 89.77%\n",
      "epoch: 15, train loss: 1.8408, accuracy: 86.42%\t\tvalid loss: 0.7737, accuracy: 92.57%\n",
      "valid sensitivity 79.81,specificity 93.31, precision 92.57\n",
      "test loss: 0.9981, accuracy: 89.77%\n",
      "epoch: 16, train loss: 0.9962, accuracy: 87.73%\t\tvalid loss: 0.4261, accuracy: 93.14%\n",
      "valid sensitivity 99.68,specificity 75.25, precision 80.78\n",
      "test loss: 0.6082, accuracy: 90.91%\n",
      "epoch: 17, train loss: 0.6377, accuracy: 89.69%\t\tvalid loss: 0.3824, accuracy: 93.71%\n",
      "valid sensitivity 98.08,specificity 80.94, precision 84.30\n",
      "test loss: 0.4024, accuracy: 89.77%\n",
      "epoch: 18, train loss: 0.8174, accuracy: 91.00%\t\tvalid loss: 0.3217, accuracy: 95.43%\n",
      "valid sensitivity 98.40,specificity 83.28, precision 85.99\n",
      "test loss: 0.3525, accuracy: 92.05%\n",
      "epoch: 19, train loss: 1.1857, accuracy: 90.83%\t\tvalid loss: 0.2479, accuracy: 95.43%\n",
      "valid sensitivity 99.04,specificity 82.27, precision 85.36\n",
      "test loss: 0.3988, accuracy: 90.91%\n",
      "epoch: 20, train loss: 1.8022, accuracy: 87.07%\t\tvalid loss: 0.5869, accuracy: 92.57%\n",
      "valid sensitivity 98.72,specificity 74.92, precision 80.42\n",
      "test loss: 0.7811, accuracy: 88.64%\n",
      "epoch: 21, train loss: 1.6368, accuracy: 85.43%\t\tvalid loss: 0.9056, accuracy: 90.29%\n",
      "valid sensitivity 99.36,specificity 70.90, precision 78.09\n",
      "test loss: 1.1677, accuracy: 85.23%\n",
      "epoch: 22, train loss: 0.9310, accuracy: 88.71%\t\tvalid loss: 0.4468, accuracy: 92.57%\n",
      "valid sensitivity 98.40,specificity 78.60, precision 82.75\n",
      "test loss: 2.3245, accuracy: 88.64%\n",
      "epoch: 23, train loss: 0.7958, accuracy: 90.34%\t\tvalid loss: 0.3489, accuracy: 96.00%\n",
      "valid sensitivity 97.44,specificity 82.94, precision 85.63\n",
      "test loss: 0.4182, accuracy: 90.91%\n",
      "epoch: 24, train loss: 0.6388, accuracy: 91.16%\t\tvalid loss: 0.4542, accuracy: 93.14%\n",
      "valid sensitivity 85.58,specificity 96.99, precision 96.74\n",
      "test loss: 0.7260, accuracy: 89.77%\n",
      "epoch: 25, train loss: 0.7337, accuracy: 92.96%\t\tvalid loss: 0.3133, accuracy: 96.57%\n",
      "valid sensitivity 97.12,specificity 88.63, precision 89.91\n",
      "test loss: 0.4001, accuracy: 93.18%\n",
      "epoch: 26, train loss: 1.6672, accuracy: 85.92%\t\tvalid loss: 0.7341, accuracy: 90.86%\n",
      "valid sensitivity 99.68,specificity 71.57, precision 78.54\n",
      "test loss: 1.3123, accuracy: 86.36%\n",
      "epoch: 27, train loss: 0.8308, accuracy: 90.34%\t\tvalid loss: 0.3100, accuracy: 94.86%\n",
      "valid sensitivity 94.55,specificity 85.95, precision 87.54\n",
      "test loss: 0.3821, accuracy: 90.91%\n",
      "epoch: 28, train loss: 0.5309, accuracy: 90.18%\t\tvalid loss: 0.2386, accuracy: 92.57%\n",
      "valid sensitivity 88.14,specificity 92.31, precision 92.28\n",
      "test loss: 0.2553, accuracy: 92.05%\n",
      "epoch: 29, train loss: 0.5988, accuracy: 90.51%\t\tvalid loss: 0.4307, accuracy: 91.43%\n",
      "valid sensitivity 85.90,specificity 95.32, precision 95.04\n",
      "test loss: 0.5321, accuracy: 88.64%\n",
      "epoch: 30, train loss: 0.6521, accuracy: 90.02%\t\tvalid loss: 0.5495, accuracy: 90.86%\n",
      "valid sensitivity 83.33,specificity 96.99, precision 96.65\n",
      "test loss: 0.6007, accuracy: 89.77%\n",
      "epoch: 31, train loss: 0.3650, accuracy: 91.65%\t\tvalid loss: 0.6002, accuracy: 92.57%\n",
      "valid sensitivity 88.46,specificity 94.98, precision 94.85\n",
      "test loss: 0.1701, accuracy: 90.91%\n",
      "epoch: 32, train loss: 0.5419, accuracy: 90.67%\t\tvalid loss: 0.7212, accuracy: 92.00%\n",
      "valid sensitivity 84.62,specificity 96.99, precision 96.70\n",
      "test loss: 0.5015, accuracy: 90.91%\n",
      "epoch: 33, train loss: 0.3695, accuracy: 92.64%\t\tvalid loss: 0.3657, accuracy: 91.43%\n",
      "valid sensitivity 92.63,specificity 92.64, precision 92.93\n",
      "test loss: 0.3214, accuracy: 87.50%\n",
      "epoch: 34, train loss: 0.5188, accuracy: 91.82%\t\tvalid loss: 0.3182, accuracy: 91.43%\n",
      "valid sensitivity 88.78,specificity 94.98, precision 94.86\n",
      "test loss: 0.5143, accuracy: 89.77%\n",
      "epoch: 35, train loss: 0.7178, accuracy: 89.85%\t\tvalid loss: 0.3524, accuracy: 92.57%\n",
      "valid sensitivity 86.54,specificity 93.31, precision 93.10\n",
      "test loss: 0.4240, accuracy: 88.64%\n",
      "epoch: 36, train loss: 0.3837, accuracy: 91.00%\t\tvalid loss: 0.3662, accuracy: 92.57%\n",
      "valid sensitivity 94.23,specificity 87.63, precision 88.82\n",
      "test loss: 0.1196, accuracy: 94.32%\n",
      "epoch: 37, train loss: 0.6910, accuracy: 88.71%\t\tvalid loss: 0.7301, accuracy: 89.14%\n",
      "valid sensitivity 85.90,specificity 91.64, precision 91.47\n",
      "test loss: 0.3350, accuracy: 88.64%\n",
      "epoch: 38, train loss: 0.5583, accuracy: 90.51%\t\tvalid loss: 0.6095, accuracy: 89.71%\n",
      "valid sensitivity 87.50,specificity 93.65, precision 93.49\n",
      "test loss: 0.3495, accuracy: 89.77%\n",
      "epoch: 39, train loss: 0.3124, accuracy: 91.33%\t\tvalid loss: 0.6431, accuracy: 91.43%\n",
      "valid sensitivity 86.22,specificity 96.66, precision 96.42\n",
      "test loss: 0.2741, accuracy: 88.64%\n",
      "epoch: 40, train loss: 0.3393, accuracy: 91.49%\t\tvalid loss: 0.6150, accuracy: 89.14%\n",
      "valid sensitivity 94.23,specificity 88.63, precision 89.63\n",
      "test loss: 0.3570, accuracy: 88.64%\n",
      "epoch: 41, train loss: 0.3553, accuracy: 89.69%\t\tvalid loss: 0.5704, accuracy: 89.71%\n",
      "valid sensitivity 88.78,specificity 90.64, precision 90.82\n",
      "test loss: 0.2197, accuracy: 87.50%\n",
      "epoch: 42, train loss: 0.5373, accuracy: 90.67%\t\tvalid loss: 0.7342, accuracy: 91.43%\n",
      "valid sensitivity 83.33,specificity 98.33, precision 98.11\n",
      "test loss: 0.5985, accuracy: 89.77%\n",
      "epoch: 43, train loss: 0.2892, accuracy: 91.49%\t\tvalid loss: 0.7079, accuracy: 93.14%\n",
      "valid sensitivity 97.44,specificity 85.28, precision 87.36\n",
      "test loss: 0.5336, accuracy: 88.64%\n",
      "epoch: 44, train loss: 0.5058, accuracy: 89.69%\t\tvalid loss: 0.8724, accuracy: 89.14%\n",
      "valid sensitivity 90.38,specificity 88.96, precision 89.52\n",
      "test loss: 0.2617, accuracy: 90.91%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-8-ca5c247ecfa1>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m    219\u001B[0m         \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m'This is No. {}  turn'\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0midx\u001B[0m\u001B[1;33m+\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    220\u001B[0m \u001B[1;31m#         train_loss_ret, train_acc_ret, val_loss_ret, val_acc_ret = main(args, idx)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 221\u001B[1;33m         \u001B[0mtrain_loss_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_acc_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_sens_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_spec_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_prec_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_loss_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_acc_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_sens_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_spec_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mval_prec_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_loss_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_acc_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_sens_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_spec_ret\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtest_prec_ret\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0midx\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    222\u001B[0m         \u001B[0mtrain_acc_list\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtrain_acc_ret\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    223\u001B[0m         \u001B[0mval_acc_list\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mval_acc_ret\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-8-ca5c247ecfa1>\u001B[0m in \u001B[0;36mmain\u001B[1;34m(args, fold_idx)\u001B[0m\n\u001B[0;32m    152\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepochs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    153\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 154\u001B[1;33m         \u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrainloader\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcriterion\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    155\u001B[0m         \u001B[0mscheduler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    156\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-8-ca5c247ecfa1>\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(args, net, trainloader, optimizer, criterion, epoch)\u001B[0m\n\u001B[0;32m     34\u001B[0m         \u001B[1;31m# backprop\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     35\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 36\u001B[1;33m         \u001B[0mloss\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     37\u001B[0m         \u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     38\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\GIN\\lib\\site-packages\\torch\\tensor.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[0;32m    243\u001B[0m                 \u001B[0mcreate_graph\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    244\u001B[0m                 inputs=inputs)\n\u001B[1;32m--> 245\u001B[1;33m         \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgradient\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    246\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    247\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0mregister_hook\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mhook\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\GIN\\lib\\site-packages\\torch\\autograd\\__init__.py\u001B[0m in \u001B[0;36mbackward\u001B[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[0;32m    145\u001B[0m     Variable._execution_engine.run_backward(\n\u001B[0;32m    146\u001B[0m         \u001B[0mtensors\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mgrad_tensors_\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mretain_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcreate_graph\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0minputs\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 147\u001B[1;33m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001B[0m\u001B[0;32m    148\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    149\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\.conda\\envs\\GIN\\lib\\site-packages\\torch\\autograd\\function.py\u001B[0m in \u001B[0;36mapply\u001B[1;34m(self, *args)\u001B[0m\n\u001B[0;32m     85\u001B[0m     \u001B[0m_is_legacy\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;32mFalse\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     86\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 87\u001B[1;33m     \u001B[1;32mdef\u001B[0m \u001B[0mapply\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     88\u001B[0m         \u001B[1;31m# _forward_cls is defined by derived class\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_forward_cls\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mbackward\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m# type: ignore\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "  \n",
    "import sys\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from dgl.data import GINDataset\n",
    "# from dataloader import GINDataLoader\n",
    "# from parser import Parser\n",
    "# from gin import GIN\n",
    "\n",
    "\n",
    "def train(args, net, trainloader, optimizer, criterion, epoch):\n",
    "    net.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    total_iters = len(trainloader)\n",
    "    # setup the offset to avoid the overlap with mouse cursor\n",
    "#     bar = tqdm(range(total_iters), unit='batch', position=2, file=sys.stdout)\n",
    "    bar = list(range(total_iters))\n",
    "\n",
    "    for pos, (graphs, labels) in zip(bar, trainloader):\n",
    "        # batch graphs will be shipped to device in forward part of model\n",
    "        labels = labels.to(device)\n",
    "        graphs = graphs.to(device)\n",
    "        feat = graphs.ndata.pop('w')\n",
    "        outputs = net(graphs, feat)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        # backprop\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # report\n",
    "#         bar.set_description('epoch-{}'.format(epoch))\n",
    "#     bar.close()\n",
    "    # the final batch will be aligned\n",
    "    running_loss = running_loss / total_iters\n",
    "\n",
    "    return running_loss\n",
    "\n",
    "\n",
    "def eval_net(args, net, dataloader, criterion):\n",
    "    net.eval()\n",
    "\n",
    "    total = 0\n",
    "    total_loss = 0\n",
    "    total_correct = 0\n",
    "    total_tp = 0\n",
    "    total_tn = 0\n",
    "    total_fp = 0\n",
    "    total_fn = 0\n",
    "\n",
    "    for data in dataloader:\n",
    "        graphs, labels = data\n",
    "        graphs = graphs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        feat = graphs.ndata.pop('w')\n",
    "        total += len(labels)\n",
    "        outputs = net(graphs, feat)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "        total_correct += (predicted == labels.data).sum().item()\n",
    "        tn, fp, fn, tp = confusion_matrix(labels.data,predicted, labels=[0,1]).ravel()\n",
    "        total_tp += tp\n",
    "        total_tn += tn\n",
    "        total_fp += fp\n",
    "        total_fn += fn\n",
    "        loss = criterion(outputs, labels)\n",
    "        # crossentropy(reduce=True) for default\n",
    "        total_loss += loss.item() * len(labels)\n",
    "\n",
    "    loss, acc = 1.0*total_loss / total, 1.0*total_correct / total\n",
    "    specificity = total_tn/(total_fp+total_tn)\n",
    "    sensitivity = total_tp/(total_tp+total_fn)\n",
    "    precision = total_tp/(total_tp+total_fp)\n",
    "    net.train()\n",
    "\n",
    "    return loss, acc, sensitivity, specificity, precision\n",
    "\n",
    "seed = 0\n",
    "batch_size = 64\n",
    "device = torch.device(\"cpu\")\n",
    "   \n",
    "num_layers = 4\n",
    "num_mlp_layers = 5\n",
    "dim_nfeats = 23\n",
    "hidden_dim = 32\n",
    "gclasses = 2\n",
    "final_dropout = 0.6\n",
    "learn_eps = False\n",
    "graph_pooling_type = 'sum'\n",
    "neighbor_pooling_type = 'sum'\n",
    "lr = 0.001\n",
    "epochs = 60\n",
    "\n",
    "def main(args, fold_idx):\n",
    "\n",
    "    # set up seeds, args.seed supported\n",
    "#     torch.manual_seed(seed=args.seed)\n",
    "#     np.random.seed(seed=args.seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "#     is_cuda = not args.disable_cuda and torch.cuda.is_available()\n",
    "\n",
    "#     if is_cuda:\n",
    "#         args.device = torch.device(\"cuda:\" + str(args.device))\n",
    "#         torch.cuda.manual_seed_all(seed=args.seed)\n",
    "#     else:\n",
    "#         args.device = torch.device(\"cpu\")\n",
    "\n",
    "#     dataset = GINDataset(args.dataset, not args.learn_eps)\n",
    "    \n",
    "    trainloader, validloader, testloader = GINDataLoader(\n",
    "        dataset, batch_size=batch_size, device=device,\n",
    "        seed=seed, shuffle=True,\n",
    "        split_name='rand', fold_idx=fold_idx).train_valid_loader()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    model = GIN(\n",
    "        num_layers, num_mlp_layers,\n",
    "        dim_nfeats, hidden_dim, gclasses,\n",
    "        final_dropout,learn_eps,\n",
    "        graph_pooling_type, neighbor_pooling_type).to(device)\n",
    "    \n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()  # defaul reduce is true\n",
    "    optimizer = optim.Adam(model.parameters(), lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n",
    "\n",
    "    # it's not cost-effective to hanle the cursor and init 0\n",
    "    # https://stackoverflow.com/a/23121189\n",
    "#     tbar = tqdm(range(epochs), unit=\"epoch\", position=3, ncols=0, file=sys.stdout)\n",
    "#     vbar = tqdm(range(epochs), unit=\"epoch\", position=4, ncols=0, file=sys.stdout)\n",
    "#     lrbar = tqdm(range(epochs), unit=\"epoch\", position=5, ncols=0, file=sys.stdout)\n",
    "\n",
    "#     for epoch, _, _ in zip(tbar, vbar, lrbar):\n",
    "    train_loss_ret, train_acc_ret, train_sens_ret, train_spec_ret, train_prec_ret  = [], [], [], [], []\n",
    "    val_loss_ret, val_acc_ret, val_sens_ret, val_spec_ret, val_prec_ret = [], [], [], [], []\n",
    "    test_loss_ret, test_acc_ret, test_sens_ret, test_spec_ret, test_prec_ret = [], [], [], [], []\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        train(args, model, trainloader, optimizer, criterion, epoch)\n",
    "        scheduler.step()\n",
    "\n",
    "        train_loss, train_acc, train_sens, train_spec, train_precision= eval_net(\n",
    "            args, model, trainloader, criterion)\n",
    "        \n",
    "        print('epoch: {}, train loss: {:.4f}, accuracy: {:.2f}%'.format(epoch+1, train_loss, 100. * train_acc), end='')\n",
    "        train_loss_ret.append(train_loss)\n",
    "        train_acc_ret.append(train_acc)\n",
    "        train_sens_ret.append(train_sens)\n",
    "        train_spec_ret.append(train_spec)\n",
    "        train_prec_ret.append(train_precision)\n",
    "\n",
    "        valid_loss, valid_acc, valid_sens, valid_spec, valid_precision = eval_net(\n",
    "            args, model, validloader, criterion)\n",
    "        print('\\t\\tvalid loss: {:.4f}, accuracy: {:.2f}%'.format(valid_loss, 100. * valid_acc))\n",
    "        print('valid sensitivity {:.2f},specificity {:.2f}, precision {:.2f}'.format(100*train_sens, 100*train_spec, 100*train_precision))\n",
    "        val_loss_ret.append(valid_loss)\n",
    "        val_acc_ret.append(valid_acc)\n",
    "        val_sens_ret.append(valid_sens)\n",
    "        val_spec_ret.append(valid_spec)\n",
    "        val_prec_ret.append(valid_precision)\n",
    "        \n",
    "        test_loss, test_acc, test_sens, test_spec, test_precision = eval_net(\n",
    "            args, model, testloader, criterion)\n",
    "        print('test loss: {:.4f}, accuracy: {:.2f}%'.format(test_loss, 100. * test_acc))\n",
    "#         print('valid sensitivity {:.2f},specificity {:.2f}, precision {:.2f}'.format(100*train_sens, 100*train_spec, 100*train_precision))\n",
    "        test_loss_ret.append(test_loss)\n",
    "        test_acc_ret.append(test_acc)\n",
    "        test_sens_ret.append(test_sens)\n",
    "        test_spec_ret.append(test_spec)\n",
    "        test_prec_ret.append(test_precision)\n",
    "        \n",
    "    return train_loss_ret, train_acc_ret, train_sens_ret, train_spec_ret, train_prec_ret, val_loss_ret, val_acc_ret, val_sens_ret, val_spec_ret, val_prec_ret, test_loss_ret, test_acc_ret, test_sens_ret, test_spec_ret, test_prec_ret\n",
    "\n",
    "#save the final result\n",
    "train_acc_list = []\n",
    "val_acc_list = []\n",
    "test_acc_list = []\n",
    "train_loss_list = []\n",
    "val_loss_list = []\n",
    "test_loss_list = []\n",
    "train_sens_list = []\n",
    "val_sens_list = []\n",
    "test_sens_list = []\n",
    "train_spec_list = []\n",
    "val_spec_list = []\n",
    "test_spec_list = []\n",
    "train_prec_list = []\n",
    "val_prec_list = []\n",
    "test_prec_list = []\n",
    "\n",
    "if __name__ == '__main__':\n",
    "#     args = Parser(description='GIN').args\n",
    "#     print('show all arguments configuration...')\n",
    "#     print(args)\n",
    "    \n",
    "    args = None\n",
    "# for thres_step in range(3):\n",
    "#     threshold = 0.2+thres_step*0.13\n",
    "\n",
    "    for idx in range(10):\n",
    "        threshold=0.8\n",
    "        seed = 0\n",
    "        dataset = MyDataset()\n",
    "        print('This is No. {}  turn'.format(idx+1))\n",
    "#         train_loss_ret, train_acc_ret, val_loss_ret, val_acc_ret = main(args, idx)\n",
    "        train_loss_ret, train_acc_ret, train_sens_ret, train_spec_ret, train_prec_ret, val_loss_ret, val_acc_ret, val_sens_ret, val_spec_ret, val_prec_ret, test_loss_ret, test_acc_ret, test_sens_ret, test_spec_ret, test_prec_ret = main(args, idx)\n",
    "        train_acc_list.append(train_acc_ret[epochs-1])\n",
    "        val_acc_list.append(val_acc_ret[epochs-1])\n",
    "        test_acc_list.append(test_acc_ret[epochs-1])\n",
    "        train_loss_list.append(train_loss_ret[epochs-1])\n",
    "        val_loss_list.append(val_loss_ret[epochs-1])\n",
    "        test_loss_list.append(test_loss_ret[epochs-1])\n",
    "        train_sens_list.append(train_sens_ret[epochs-1])\n",
    "        val_sens_list.append(val_sens_ret[epochs-1])\n",
    "        test_sens_list.append(test_sens_ret[epochs-1])\n",
    "        train_spec_list.append(train_spec_ret[epochs-1])\n",
    "        val_spec_list.append(val_spec_ret[epochs-1])\n",
    "        test_spec_list.append(test_spec_ret[epochs-1])\n",
    "        train_prec_list.append(train_prec_ret[epochs-1])\n",
    "        val_prec_list.append(val_prec_ret[epochs-1])\n",
    "        test_prec_list.append(test_prec_ret[epochs-1])\n",
    "        \n",
    "        plt.subplot(211)\n",
    "        plt.plot(list(range(epochs)), train_loss_ret, color='r')\n",
    "        plt.plot(list(range(epochs)), val_loss_ret, color='g')\n",
    "        plt.subplot(212)\n",
    "        plt.plot(list(range(epochs)), train_acc_ret, color='r')\n",
    "        plt.plot(list(range(epochs)), val_acc_ret, color='g')\n",
    "        plt.show()\n",
    "\n",
    "    print('Final result: valid accuracy {:.2f}, valid sensitivity {:.2f}, valid specificity {:.2f}, valid precision {:.2f}'.format(10*sum(val_acc_list), 10*sum(val_sens_list), 10*sum(val_spec_list), 10*sum(val_prec_list)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d737fd7a955a03ef172416a81dd935dff74d542aae30bb92c784f41fa1311fe5"
  },
  "kernelspec": {
   "display_name": "Python GIN",
   "language": "python",
   "name": "gin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}